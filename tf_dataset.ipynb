{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinCheungS/Deep_Learning_TF2/blob/master/tf_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mDX-p8-3CMm",
        "colab_type": "text"
      },
      "source": [
        "# tf.Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8ckDL_a3QXU",
        "colab_type": "text"
      },
      "source": [
        "如果需要训练的数据不大，不到1G，可以全部读入内存中训练，效率最高。\n",
        "\n",
        "但如果训练的数据很大，超过10G，无法一次载入内存，需要分批逐渐训练"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iihOkoPufU9A",
        "colab_type": "text"
      },
      "source": [
        "![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-1/20200505050006.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyaC7AmE32ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTbxvBlm3GHL",
        "colab_type": "text"
      },
      "source": [
        "## tf.data API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTCWpb654GTN",
        "colab_type": "text"
      },
      "source": [
        "### 生成dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNnAIv913E-c",
        "colab_type": "code",
        "outputId": "9d154571-58ce-41f9-edf4-38b295d2d58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# 生成dataset,tf.Tensor(0到2, shape=(), dtype=int64)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(np.arange(3))\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-W7_L4N4aOW",
        "colab_type": "code",
        "outputId": "6aaa8972-d636-47f3-c33b-ed747cc75d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# 利用两个列表构建dataset\n",
        "x = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "y = np.array(['cat', 'dog', 'fox'])\n",
        "dataset3 = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "for item_x, item_y in dataset3:\n",
        "    print(item_x.numpy(), item_y.numpy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2] b'cat'\n",
            "[3 4] b'dog'\n",
            "[5 6] b'fox'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hsyjz3v4yot",
        "colab_type": "code",
        "outputId": "a7a990d1-79b4-4a2e-c14a-22421be54656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# 利用字典构建dataset\n",
        "dataset4 = tf.data.Dataset.from_tensor_slices({\"feature\": x,\"label\": y})\n",
        "for item in dataset4:\n",
        "    print(item[\"feature\"].numpy(), item[\"label\"].numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2] b'cat'\n",
            "[3 4] b'dog'\n",
            "[5 6] b'fox'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXkWsuer5A2l",
        "colab_type": "text"
      },
      "source": [
        "### dataset.repeat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL4bUDzm47tt",
        "colab_type": "code",
        "outputId": "1b28cf41-d57e-4dd7-ea3c-4d25b089ddf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# dataset重复多少次\n",
        "dataset = tf.data.Dataset.from_tensor_slices(np.arange(2))\n",
        "dataset = dataset.repeat(3)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6iIrXyB5rir",
        "colab_type": "text"
      },
      "source": [
        "### dataset.batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezT159SJ5pKn",
        "colab_type": "code",
        "outputId": "44ab2131-3264-4830-b3fd-a4c2c8742299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# 分成多少份\n",
        "dataset = tf.data.Dataset.from_tensor_slices(np.arange(1,10))\n",
        "dataset = dataset.batch(3)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
            "tf.Tensor([4 5 6], shape=(3,), dtype=int64)\n",
            "tf.Tensor([7 8 9], shape=(3,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzxiz1286Dom",
        "colab_type": "text"
      },
      "source": [
        "### dataset.interleave"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSdMRaJf5eCV",
        "colab_type": "code",
        "outputId": "679e878b-631f-4cde-a27e-36f5507fde52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\"\"\" 一对多,类似flaten\n",
        "1. 从Dataset中取出cycle_length个element，\n",
        "2. 得到cycle_length个新的Dataset对象。,对element应用map_func\n",
        "3. 从新生成的Dataset中取数据，每个Dataset对象一次取block_length个数据。\n",
        "4. 当新生成的某个Dataset的对象取尽时，从原Dataset中再取cycle_length个element，\n",
        "5. 然后apply map_func，以此类推。\n",
        "\"\"\"\n",
        "dataset2 = dataset.interleave(\n",
        "    lambda v: tf.data.Dataset.from_tensor_slices(v), # map_fn\n",
        "    cycle_length = 2, # cycle_length\n",
        "    block_length = 2, # block_length\n",
        ")\n",
        "for item in dataset2:\n",
        "    print(item)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBUu4WIneh9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "17eb0cd8-e956-46e0-c127-f68c2815ef37"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices([[\"hello world\",\"hello China\"],[\"hello world\",\"hello China\"]])\n",
        "ds_interleave = ds.interleave(lambda x:tf.data.Dataset.from_tensor_slices(x))\n",
        "for x in ds_interleave:\n",
        "    print(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'hello world', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello world', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello China', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello China', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScVVuOOoBc3L",
        "colab_type": "text"
      },
      "source": [
        "## Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkST_g1qBb4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "87e185a7-e7a1-4200-87ed-100669c69e0d"
      },
      "source": [
        "# 详细操作https://tensorflow.google.cn/tutorials/load_data/pandas_dataframe\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets \n",
        "import pandas as pd\n",
        "iris = datasets.load_iris()\n",
        "dfiris = pd.DataFrame(iris[\"data\"],columns = iris.feature_names)\n",
        "dfiris.head()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0                5.1               3.5                1.4               0.2\n",
              "1                4.9               3.0                1.4               0.2\n",
              "2                4.7               3.2                1.3               0.2\n",
              "3                4.6               3.1                1.5               0.2\n",
              "4                5.0               3.6                1.4               0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7fo_9yTBzGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ca1bdc29-2817-4446-9ced-da42309e03eb"
      },
      "source": [
        "ds2 = tf.data.Dataset.from_tensor_slices((dfiris.values,iris[\"target\"]))\n",
        "\n",
        "for features,label in ds2.take(3):\n",
        "    print(features,label)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJUH0NqtO2XP",
        "colab_type": "text"
      },
      "source": [
        "## CSV操作"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkiWVRkK_nCN",
        "colab_type": "text"
      },
      "source": [
        "### 转成csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CFLzHp6_mcS",
        "colab_type": "code",
        "outputId": "0606e08d-e865-444f-d380-de0e101cfa0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 加载california_housing数据\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "housing = fetch_california_housing()\n",
        "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state = 7)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state = 11)\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "print(x_train_scaled.shape, y_train.shape,x_valid_scaled.shape, y_valid.shape,x_test_scaled.shape, y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11610, 8) (11610,) (3870, 8) (3870,) (5160, 8) (5160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIyUgKGt9Qy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "   output_dir = 输出路径,data =原始数据,name_prefix =前缀名字,\n",
        "   header = feature的label, n_parts = 分成几份\n",
        "\"\"\"\n",
        "def save_to_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
        "    # 拼接输出路径和输出文件名字\n",
        "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
        "    # 存储路径名字\n",
        "    file_dir_names = []\n",
        "    # 生成data长度个index,并分成n_parts份\n",
        "    index_n_parts = np.array_split(np.arange(len(data)), n_parts)\n",
        "    \"\"\"枚举index_n_parts, 得到序号和dataset的index\"\"\"\n",
        "    for file_idx, row_indices in enumerate(index_n_parts):\n",
        "        # 对每一份n_parts命名,例如train_01,valid_01\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        # 将文件名追加到file_dir_names\n",
        "        file_dir_names.append(part_csv)\n",
        "        \"\"\"将数据写入每一份n_parts\"\"\"\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            # 如果header不是空的,写入header到第一行\n",
        "            if header is not None:\n",
        "                f.write(header + \"\\n\")\n",
        "            # 将这一份的n_parts写入train/valid/test_0x.csv中\n",
        "            for row_index in row_indices:\n",
        "                # 做字符串化处理,转成字符串\n",
        "                f.write(\",\".join([repr(col) for col in data[row_index]]))\n",
        "                f.write('\\n')\n",
        "    # 返回路径名\n",
        "    return file_dir_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KIHPF-1HX2L",
        "colab_type": "code",
        "outputId": "27b510ed-8af9-475c-9c88-f357a1303e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# 不存在,则创建数据文件夹\n",
        "output_dir = \"generate_csv\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "# 拼接x和y,np.row_stack((a,b)),np.column_stack((a,b))\n",
        "train_data = np.column_stack([x_train_scaled, y_train])\n",
        "valid_data = np.column_stack([x_valid_scaled, y_valid])\n",
        "test_data = np.column_stack([x_test_scaled, y_test])\n",
        "\n",
        "# 把名字转成csv格式\n",
        "header_cols = housing.feature_names + [\"price_y\"]\n",
        "header_str = \",\".join(header_cols)\n",
        "\n",
        "# 生成csv文件\n",
        "train_filenames = save_to_csv(output_dir, train_data, \"train\",header_str, n_parts=20)\n",
        "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\",header_str, n_parts=10)\n",
        "test_filenames = save_to_csv(output_dir, test_data, \"test\",header_str, n_parts=10)\n",
        "\n",
        "print(train_filenames)\n",
        "print(valid_filenames)\n",
        "print(test_filenames)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['generate_csv/train_00.csv', 'generate_csv/train_01.csv', 'generate_csv/train_02.csv', 'generate_csv/train_03.csv', 'generate_csv/train_04.csv', 'generate_csv/train_05.csv', 'generate_csv/train_06.csv', 'generate_csv/train_07.csv', 'generate_csv/train_08.csv', 'generate_csv/train_09.csv', 'generate_csv/train_10.csv', 'generate_csv/train_11.csv', 'generate_csv/train_12.csv', 'generate_csv/train_13.csv', 'generate_csv/train_14.csv', 'generate_csv/train_15.csv', 'generate_csv/train_16.csv', 'generate_csv/train_17.csv', 'generate_csv/train_18.csv', 'generate_csv/train_19.csv']\n",
            "['generate_csv/valid_00.csv', 'generate_csv/valid_01.csv', 'generate_csv/valid_02.csv', 'generate_csv/valid_03.csv', 'generate_csv/valid_04.csv', 'generate_csv/valid_05.csv', 'generate_csv/valid_06.csv', 'generate_csv/valid_07.csv', 'generate_csv/valid_08.csv', 'generate_csv/valid_09.csv']\n",
            "['generate_csv/test_00.csv', 'generate_csv/test_01.csv', 'generate_csv/test_02.csv', 'generate_csv/test_03.csv', 'generate_csv/test_04.csv', 'generate_csv/test_05.csv', 'generate_csv/test_06.csv', 'generate_csv/test_07.csv', 'generate_csv/test_08.csv', 'generate_csv/test_09.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81umhEPvMcPP",
        "colab_type": "text"
      },
      "source": [
        "### 读取csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSnaYzdqspJL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "811703eb-e444-48db-ad10-67287a4b3858"
      },
      "source": [
        "\"\"\"# https://tensorflow.google.cn/tutorials/load_data/csv\n",
        "ds4 = tf.data.experimental.make_csv_dataset(\n",
        "      file_pattern = [\"/content/generate_csv/train_00.csv\"],\n",
        "      batch_size=300, \n",
        "      label_name=\"MidianHouseValue\",\n",
        "      na_value=\"\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True)\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# https://tensorflow.google.cn/tutorials/load_data/csv\\nds4 = tf.data.experimental.make_csv_dataset(\\n      file_pattern = [\"/content/generate_csv/train_00.csv\"],\\n      batch_size=300, \\n      label_name=\"MidianHouseValue\",\\n      na_value=\"\",\\n      num_epochs=1,\\n      ignore_errors=True)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OginTPTAdWg",
        "colab_type": "code",
        "outputId": "59b4e81f-fa14-4fa8-da78-348ee5b9c690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"核心方法: tf.io.decode_csv(str, record_defaults)\"\"\"\n",
        "# 定义了数据\n",
        "sample_str = '1,2,3'\n",
        "# 定义了数据类型\n",
        "record_defaults = [\n",
        "    tf.constant(0,dtype=tf.int32),\n",
        "    tf.constant(1.0,dtype=tf.float32),\n",
        "    tf.constant(\"0\",dtype=tf.string),]\n",
        "parsed_fields = tf.io.decode_csv(sample_str, record_defaults)\n",
        "print(parsed_fields)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>, <tf.Tensor: shape=(), dtype=string, numpy=b'3'>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpdC-kyzPLJm",
        "colab_type": "code",
        "outputId": "8ed39ec0-bbc7-4bf2-a9a9-89a3efbfb6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# 1. filename -> dataset\n",
        "# 2. read file -> dataset -> datasets -> merge\n",
        "# 3. parse csv\n",
        "\n",
        "def parse_csv_line(line, n_fields = 9):\n",
        "    defs = [tf.constant(np.nan)] * n_fields\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(parsed_fields[0:-1])\n",
        "    y = tf.stack(parsed_fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filenames, n_readers=5,batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    # 将文件路径封装在tf.datset\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    # dataset = dataset.repeat()\n",
        "    # 根据文件名, 一行一行读取数据, 可视化用take\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1),# 跳过header\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    # 打乱数据\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    # 调用parse_csv_line函数, 将每一行的每一个数据分开,分成x,y\n",
        "    dataset = dataset.map(parse_csv_line,num_parallel_calls=n_parse_threads)\n",
        "    # 规定多少数据为一批\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "train_set = csv_reader_dataset(train_filenames, batch_size=3)\n",
        "for x_batch, y_batch in train_set.take(1):\n",
        "    print(\"x:\\n\",x_batch)\n",
        "    print(\"y:\\n\",y_batch)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            " tf.Tensor(\n",
            "[[-1.0591781e+00  1.3935647e+00 -2.6331969e-02 -1.1006760e-01\n",
            "  -6.1381990e-01 -9.6959352e-02  3.2471311e-01 -3.7477244e-02]\n",
            " [ 4.0127665e-01 -9.2934215e-01 -5.3330503e-02 -1.8659453e-01\n",
            "   6.5456617e-01  2.6434466e-02  9.3125278e-01 -1.4406418e+00]\n",
            " [ 4.9710345e-02 -8.4924191e-01 -6.2146995e-02  1.7878747e-01\n",
            "  -8.0253541e-01  5.0660671e-04  6.4664572e-01 -1.1060793e+00]], shape=(3, 8), dtype=float32)\n",
            "y:\n",
            " tf.Tensor(\n",
            "[[0.672]\n",
            " [2.512]\n",
            " [2.286]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltjLXZIER2PS",
        "colab_type": "code",
        "outputId": "13ab4492-964e-41ce-8589-6e8dc8e91103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "\"\"\"例子\"\"\"\n",
        "# 生成完整数据\n",
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames,batch_size = batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames,batch_size = batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames,batch_size = batch_size)\n",
        "# 训练\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu',input_shape=[8]),#修改输入维度\n",
        "    keras.layers.Dense(1),])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=2, min_delta=1e-2)]\n",
        "history = model.fit(train_set, # train_set 包含x和y\n",
        "                    validation_data = valid_set, # valid_set 包含x和y\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)\n",
        "# 衡量\n",
        "model.evaluate(test_set)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 1.0703 - val_loss: 0.5627\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5387 - val_loss: 0.4848\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4423 - val_loss: 0.4385\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4079 - val_loss: 0.4187\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3974 - val_loss: 0.4065\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3895 - val_loss: 0.3958\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3828 - val_loss: 0.3913\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3784 - val_loss: 0.3841\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3740 - val_loss: 0.3813\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3698 - val_loss: 0.3809\n",
            "162/162 [==============================] - 0s 2ms/step - loss: 0.3884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3884371519088745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOnvSsEopGPY",
        "colab_type": "text"
      },
      "source": [
        "## tfrecord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAejRKOksu79",
        "colab_type": "text"
      },
      "source": [
        "### 基础API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eXfkJUKfiZC",
        "colab_type": "code",
        "outputId": "2586fcbf-ffd8-4cb9-ddf2-5a3b6db13b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "\"\"\"\n",
        "tfrecord 文件格式\n",
        "-> tf.train.Example\n",
        "   -> tf.train.Features -> {\"key\": tf.train.Feature}\n",
        "      -> tf.train.Feature -> tf.train.ByteList/FloatList/Int64List\n",
        "\"\"\"\n",
        "# 包装成 tf.train.ByteList/FloatList/Int64List\n",
        "favorite_books = [name.encode('utf-8') for name in [\"machine learning\", \"cc150\"]]\n",
        "favorite_books_bytelist = tf.train.BytesList(value = favorite_books)\n",
        "hours_floatlist = tf.train.FloatList(value = [15.5, 9.5, 7.0, 8.0])\n",
        "age_int64list = tf.train.Int64List(value = [42])\n",
        "\n",
        "# 包装成 train.Features 由 train.Feature\n",
        "features = tf.train.Features(\n",
        "    feature = {\n",
        "        \"favorite_books\": tf.train.Feature(\n",
        "            bytes_list = favorite_books_bytelist),\n",
        "        \"hours\": tf.train.Feature(\n",
        "            float_list = hours_floatlist),\n",
        "        \"age\": tf.train.Feature(int64_list = age_int64list),\n",
        "    }\n",
        ")\n",
        "\n",
        "# 包装成tf.train.Example\n",
        "example = tf.train.Example(features=features)\n",
        "# 压缩成16进制\n",
        "serialized_example = example.SerializeToString()\n",
        "\n",
        "print(type(favorite_books_bytelist))\n",
        "print(type(features))\n",
        "print(type(example))\n",
        "print(serialized_example)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.core.example.feature_pb2.BytesList'>\n",
            "<class 'tensorflow.core.example.feature_pb2.Features'>\n",
            "<class 'tensorflow.core.example.example_pb2.Example'>\n",
            "b'\\n\\\\\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01*\\n\\x1d\\n\\x05hours\\x12\\x14\\x12\\x12\\n\\x10\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xe0@\\x00\\x00\\x00A'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg_bZRp8ACQS",
        "colab_type": "code",
        "outputId": "e831a2ba-d7f4-48d0-87de-cae94678d4d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "output_dir = 'tfrecord_basic'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "\"\"\"保存成tfrecord文件,读取tfrecord文件,解析tfrecord文件\"\"\"\n",
        "# 保存成tfrecord文件\n",
        "filename_fullpath = os.path.join(\"tfrecord_basic\", \"test.tfrecords\")\n",
        "with tf.io.TFRecordWriter(filename_fullpath) as writer:\n",
        "        writer.write(serialized_example)\n",
        "\n",
        "# 读取读取tfrecord文件,16进制\n",
        "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
        "# 解析的规则, 字典的形式\n",
        "expected_features = {\n",
        "    \"favorite_books\": tf.io.VarLenFeature(dtype = tf.string),\n",
        "    \"hours\": tf.io.VarLenFeature(dtype = tf.float32),\n",
        "    \"age\": tf.io.FixedLenFeature([], dtype = tf.int64),}\n",
        "\n",
        "for serialized_example_tensor in dataset:\n",
        "    # 将规则和数据传入,解析tfrecord文件\n",
        "    example = tf.io.parse_single_example(serialized_example_tensor,expected_features)\n",
        "    books = tf.sparse.to_dense(example[\"favorite_books\"])\n",
        "    for book in books:\n",
        "        print(book.numpy().decode(\"UTF-8\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine learning\n",
            "cc150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2ho8l-wAoOU",
        "colab_type": "code",
        "outputId": "110bb97a-7c87-44b0-9c5d-5c16cfaf161c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"保存压缩tfrecord,读取压缩tfrecord,解析tfrecord\"\"\"\n",
        "filename_fullpath_zip = filename_fullpath + '.zip'\n",
        "options = tf.io.TFRecordOptions(compression_type = \"GZIP\")\n",
        "# 传入TFRecordOptions,比如compression_type = \"GZIP\"\n",
        "with tf.io.TFRecordWriter(filename_fullpath_zip, options) as writer:\n",
        "        writer.write(serialized_example)\n",
        "\n",
        "# 读取tfrecord的压缩文件\n",
        "dataset_zip = tf.data.TFRecordDataset([filename_fullpath_zip],compression_type= \"GZIP\")\n",
        "for serialized_example_tensor in dataset_zip:\n",
        "    example = tf.io.parse_single_example(serialized_example_tensor,expected_features)\n",
        "    books = tf.sparse.to_dense(example[\"favorite_books\"])\n",
        "    for book in books:\n",
        "        print(book.numpy().decode(\"UTF-8\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine learning\n",
            "cc150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N6vfdb6GYc5",
        "colab_type": "text"
      },
      "source": [
        "### 转成tfrecord"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRrbaab5DHlw",
        "colab_type": "code",
        "outputId": "57833692-4638-4ffe-aa4b-32fa0cd39b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# 读取train,validation,test数据的文件路径\n",
        "source_dir = \"./generate_csv/\"\n",
        "def get_filenames_by_prefix(source_dir, prefix_name):\n",
        "    all_files = os.listdir(source_dir)\n",
        "    results = []\n",
        "    for filename in all_files:\n",
        "        if filename.startswith(prefix_name):\n",
        "            results.append(os.path.join(source_dir, filename))\n",
        "    return results\n",
        "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
        "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
        "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
        "print(train_filenames,\"\\n\",valid_filenames,\"\\n\",test_filenames)\n",
        "\n",
        "# 由文件路径读取数据,返回x[,8],y[,1]\n",
        "def parse_csv_line(line, n_fields = 9): # n_fields需要修改\n",
        "    defs = [tf.constant(np.nan)] * n_fields\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(parsed_fields[0:-1])\n",
        "    y = tf.stack(parsed_fields[-1:])\n",
        "    return x, y\n",
        "def csv_reader_dataset(filenames, n_readers=5,batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
        "        cycle_length = n_readers)\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_csv_line,num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames,batch_size = batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames,batch_size = batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames,batch_size = batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./generate_csv/train_17.csv', './generate_csv/train_14.csv', './generate_csv/train_01.csv', './generate_csv/train_16.csv', './generate_csv/train_19.csv', './generate_csv/train_09.csv', './generate_csv/train_18.csv', './generate_csv/train_05.csv', './generate_csv/train_04.csv', './generate_csv/train_07.csv', './generate_csv/train_08.csv', './generate_csv/train_10.csv', './generate_csv/train_06.csv', './generate_csv/train_13.csv', './generate_csv/train_12.csv', './generate_csv/train_02.csv', './generate_csv/train_00.csv', './generate_csv/train_03.csv', './generate_csv/train_15.csv', './generate_csv/train_11.csv'] \n",
            " ['./generate_csv/valid_00.csv', './generate_csv/valid_02.csv', './generate_csv/valid_06.csv', './generate_csv/valid_09.csv', './generate_csv/valid_08.csv', './generate_csv/valid_04.csv', './generate_csv/valid_01.csv', './generate_csv/valid_03.csv', './generate_csv/valid_07.csv', './generate_csv/valid_05.csv'] \n",
            " ['./generate_csv/test_06.csv', './generate_csv/test_09.csv', './generate_csv/test_03.csv', './generate_csv/test_08.csv', './generate_csv/test_02.csv', './generate_csv/test_00.csv', './generate_csv/test_04.csv', './generate_csv/test_01.csv', './generate_csv/test_07.csv', './generate_csv/test_05.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmuYg0TGu-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FloatList->features->example->SerializeToString\n",
        "def serialize_example(x, y):\n",
        "    \"\"\"Converts x, y to tf.train.Example and serialize\"\"\"\n",
        "    input_feautres = tf.train.FloatList(value = x)\n",
        "    label = tf.train.FloatList(value = y)\n",
        "    features = tf.train.Features(\n",
        "        feature = {\n",
        "            \"input_features\": tf.train.Feature(\n",
        "                float_list = input_feautres),\n",
        "            \"label\": tf.train.Feature(float_list = label)\n",
        "        }\n",
        "    )\n",
        "    example = tf.train.Example(features = features)\n",
        "    return example.SerializeToString()\n",
        "\n",
        "# n_shards=要生成几个tfrecord文件, steps_per_shard=每个n_shards要读取几个batch\n",
        "def csv_dataset_to_tfrecords(base_filename, dataset,n_shards,steps_per_shard,\n",
        "                             compression_type = None):\n",
        "    options = tf.io.TFRecordOptions(compression_type = compression_type)\n",
        "    all_filenames = []\n",
        "    for shard_id in range(n_shards):\n",
        "        # 生成的tfrecord的名字\n",
        "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(base_filename, shard_id, n_shards)\n",
        "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
        "            # 每个n_shards要读取几个batch,SerializeToString写入\n",
        "            for x_batch, y_batch in dataset.take(steps_per_shard):\n",
        "                for x_example, y_example in zip(x_batch, y_batch):\n",
        "                    writer.write(serialize_example(x_example, y_example))\n",
        "        all_filenames.append(filename_fullpath)\n",
        "    return all_filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhH8kIbwLEpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_shards = 10\n",
        "train_steps_per_shard = int(np.ceil(np.ceil(11610/batch_size)/n_shards))\n",
        "valid_steps_per_shard = int(np.ceil(np.ceil(3880/batch_size)/n_shards))\n",
        "test_steps_per_shard = int(np.ceil(np.ceil(5170/batch_size)/n_shards))\n",
        "# 生成tfrecord文件\n",
        "output_dir = \"generate_tfrecords\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, valid_steps_per_shard, None)\n",
        "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, test_steps_per_shard, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92yUSKJqPkha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 生成tfrecord压缩文件\n",
        "output_dir = \"generate_tfrecords_zip\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, train_steps_per_shard,\n",
        "    compression_type = \"GZIP\")\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, valid_steps_per_shard,\n",
        "    compression_type = \"GZIP\")\n",
        "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, test_steps_per_shard,\n",
        "    compression_type = \"GZIP\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqIMt_gMSiH3",
        "colab_type": "text"
      },
      "source": [
        "### 读取rfrecord"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_yjCUFFQE0n",
        "colab_type": "code",
        "outputId": "b2933845-6fd0-47d2-eda3-cc560d791ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "expected_features = {\n",
        "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
        "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
        "}\n",
        "# tf.io.parse_single_example解析16进制的tfrecord数据\n",
        "def parse_example(serialized_example):\n",
        "    example = tf.io.parse_single_example(serialized_example,expected_features)\n",
        "    return example[\"input_features\"], example[\"label\"]\n",
        "\n",
        "def tfrecords_reader_dataset(filenames, n_readers=5,batch_size=32,\n",
        "                             n_parse_threads=5, shuffle_buffer_size=10000,\n",
        "                             compression_type = None):\n",
        "    # 1.路径文件名写入dataset\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    # 2.dataset.interleave()读取所有的数据\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TFRecordDataset(filename, compression_type = compression_type),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    # 用设置好的字典,解析16进制的tfrecord数据\n",
        "    dataset = dataset.map(parse_example, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filenames,batch_size = 4,\n",
        "                                           compression_type = \"GZIP\")\n",
        "for x_batch, y_batch in tfrecords_train.take(1):\n",
        "    print(x_batch)\n",
        "    print(y_batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.15782312  0.4323619   0.3379948  -0.01588031 -0.37338907 -0.05305246\n",
            "   0.80061346 -1.2359096 ]\n",
            " [ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
            "  -0.69240725  0.72652334]\n",
            " [-0.66722274 -0.04823952  0.34529406  0.53826684  1.8521839  -0.06112538\n",
            "  -0.8417093   1.5204847 ]\n",
            " [ 0.15782312  0.4323619   0.3379948  -0.01588031 -0.37338907 -0.05305246\n",
            "   0.80061346 -1.2359096 ]], shape=(4, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[3.169]\n",
            " [2.419]\n",
            " [1.59 ]\n",
            " [3.169]], shape=(4, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8ZjEb6hQ9pI",
        "colab_type": "code",
        "outputId": "d96c8519-225a-400b-ff4a-d80e193226ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        }
      },
      "source": [
        "batch_size = 32\n",
        "tfrecords_train_set = tfrecords_reader_dataset(train_tfrecord_filenames, batch_size = batch_size,compression_type = \"GZIP\" )\n",
        "tfrecords_valid_set = tfrecords_reader_dataset(valid_tfrecord_filenames, batch_size = batch_size,compression_type = \"GZIP\")\n",
        "tfrecords_test_set = tfrecords_reader_dataset(test_tfrecord_fielnames, batch_size = batch_size,compression_type = \"GZIP\")\n",
        "tf.keras.backend.clear_session()\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu',\n",
        "                       input_shape=[8]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "    patience=5, min_delta=1e-2)]\n",
        "\n",
        "history = model.fit(tfrecords_train_set,\n",
        "                    validation_data = tfrecords_valid_set,\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)\n",
        "model.evaluate(tfrecords_test_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.8506 - val_loss: 0.5445\n",
            "Epoch 2/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.4439 - val_loss: 0.4798\n",
            "Epoch 3/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.4145 - val_loss: 0.4528\n",
            "Epoch 4/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3973 - val_loss: 0.4392\n",
            "Epoch 5/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3870 - val_loss: 0.4348\n",
            "Epoch 6/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3792 - val_loss: 0.4207\n",
            "Epoch 7/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3723 - val_loss: 0.4117\n",
            "Epoch 8/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3664 - val_loss: 0.4073\n",
            "Epoch 9/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3627 - val_loss: 0.4011\n",
            "Epoch 10/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3581 - val_loss: 0.3998\n",
            "Epoch 11/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3546 - val_loss: 0.3935\n",
            "Epoch 12/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3526 - val_loss: 0.3863\n",
            "Epoch 13/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3484 - val_loss: 0.3867\n",
            "Epoch 14/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3453 - val_loss: 0.3814\n",
            "Epoch 15/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3431 - val_loss: 0.3801\n",
            "Epoch 16/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3403 - val_loss: 0.3843\n",
            "Epoch 17/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3382 - val_loss: 0.3764\n",
            "Epoch 18/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.3735\n",
            "Epoch 19/100\n",
            "370/370 [==============================] - 1s 2ms/step - loss: 0.3339 - val_loss: 0.3779\n",
            "170/170 [==============================] - 0s 1ms/step - loss: 0.3663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36626240611076355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    }
  ]
}